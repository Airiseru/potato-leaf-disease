{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf854fff",
      "metadata": {
        "id": "bf854fff"
      },
      "source": [
        "# Deep Learning with Transformer Method Demo Code\n",
        "\n",
        "This notebook will contain the code necessary to generate the predictions file using preprocessing techniques and model generated from the `develop_deep.ipynb` notebook.\n",
        "\n",
        "For this, a model named DINOv2 was used to classify the images after training it with the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc91b269",
      "metadata": {
        "id": "fc91b269"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7eb79d32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eb79d32",
        "outputId": "bccadc83-b121-422b-c6c8-efe55260bac3"
      },
      "outputs": [],
      "source": [
        "# %pip install tensorflow transformers torch torchvision torchaudio scikit-learn opencv-python numpy pickle5 tqdm -q\n",
        "\n",
        "# To hide warnings produced by different packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89f8eba",
      "metadata": {
        "id": "e89f8eba"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "099fc2f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099fc2f1",
        "outputId": "7232408b-5b55-42a3-d9c2-6aebd953d52f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from copy import deepcopy\n",
        "import typing\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "import cv2.typing as cv_typing\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import csv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72db4e6",
      "metadata": {
        "id": "e72db4e6"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "43b084a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b084a2",
        "outputId": "3333aaa9-80fd-4131-8e62-40759e11d059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1f8bba0c870>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classes = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"]\n",
        "\n",
        "# Path of where the train images are located\n",
        "# img_dir = \"/content/drive/MyDrive/DCS/CS180/Project/potato_test\"\n",
        "img_dir = \"../data/potato_test\" # if local\n",
        "\n",
        "# Path for final model\n",
        "# model_dir = \"/content/drive/MyDrive/DCS/CS180/Project/models\"\n",
        "model_dir = \"../models\" # if local\n",
        "\n",
        "# Other constants\n",
        "ORIG_IMG_SIZE = (1500,1500)\n",
        "BATCH_SIZE = 8\n",
        "seed_value = 42\n",
        "RESIZE_IMG = (420, 420)\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "PObp4aCxBkos",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PObp4aCxBkos",
        "outputId": "e63dee12-b853-45bd-b225-75723421ac3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9dab0f39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dab0f39",
        "outputId": "26a28784-6213-4779-c9bb-d94907d6300a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x NestedTensorBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): MemEffAttention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the vits14 version of dinov2\n",
        "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "dino_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "N15saeTzVv1a",
      "metadata": {
        "id": "N15saeTzVv1a"
      },
      "outputs": [],
      "source": [
        "class DinoVisionTransformerClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DinoVisionTransformerClassifier, self).__init__()\n",
        "        self.transformer = deepcopy(dino_model)\n",
        "        self.classifier = nn.Sequential(nn.Dropout(0.7), nn.ReLU(), nn.Linear(in_features=384, out_features=len(classes), bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)\n",
        "        x = self.transformer.norm(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aa83af65",
      "metadata": {
        "id": "aa83af65"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "with open(Path(f\"{model_dir}/dino_model_final.pth\"), 'rb') as file:\n",
        "    torch.serialization.add_safe_globals([DinoVisionTransformerClassifier])\n",
        "    loaded_model = torch.load(f\"{model_dir}/dino_model_final.pth\", map_location=device, weights_only=False)\n",
        "    loaded_model = loaded_model.to(device)\n",
        "    loaded_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9be6e4",
      "metadata": {
        "id": "db9be6e4"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904fd91b",
      "metadata": {
        "id": "904fd91b"
      },
      "source": [
        "### Loading Data\n",
        "\n",
        "`load_images` takes the directory where the test images are located and loads them into a program as a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1460c1d8",
      "metadata": {
        "id": "1460c1d8"
      },
      "outputs": [],
      "source": [
        "def load_images(\n",
        "    file_path: str = \"./potato_test\",\n",
        "    resize_dim: tuple[int, int] = (518, 518),\n",
        ") -> list[cv_typing.MatLike]:\n",
        "    # Get folder\n",
        "    dir = Path(file_path)\n",
        "\n",
        "    # Check if directory\n",
        "    if not dir.is_dir():\n",
        "        raise Exception(\"Please enter a valid directory\")\n",
        "\n",
        "    # Get all images in the dir\n",
        "    imgs = [os.path.join(dir, f) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
        "\n",
        "    # Variable for final array\n",
        "    final_imgs: list[cv_typing.MatLike] = []\n",
        "    final_filenames: list[str] = []\n",
        "\n",
        "    try:\n",
        "        for img_path in imgs:\n",
        "            img_loaded: Image.Image = image.load_img(img_path, target_size=ORIG_IMG_SIZE)\n",
        "            img_array: np.ndarray[typing.Any, typing.Any] = image.img_to_array(img_loaded)\n",
        "            img_array = cv2.resize(img_array, resize_dim)\n",
        "            final_imgs.append(img_array)\n",
        "\n",
        "            filename = Path(img_path).name\n",
        "            final_filenames.append(filename)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load images: {e}\")\n",
        "\n",
        "    return final_imgs, final_filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a10643c",
      "metadata": {
        "id": "3a10643c"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "The `preprocess_img` takes a list of images, preprocessing the images by simply resizing them, and turns it into a `DataLoader` for the model to process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Zes8qAAQIIEl",
      "metadata": {
        "id": "Zes8qAAQIIEl"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(RESIZE_IMG),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class PotatoLeafDisease(Dataset):\n",
        "    def __init__(self, imgs: list[np.ndarray], transforms: transforms.Compose):\n",
        "        self.imgs = imgs\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        img = self.imgs[idx]\n",
        "\n",
        "        # Convert numpy image to PIL Image\n",
        "        image = Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "        # Apply transform\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eb1ac5b7",
      "metadata": {
        "id": "eb1ac5b7"
      },
      "outputs": [],
      "source": [
        "def preprocess_img(\n",
        "    imgs: list[np.ndarray[typing.Any, typing.Any]] = [],\n",
        ") -> DataLoader:\n",
        "\n",
        "    leaves_data = PotatoLeafDisease(imgs, transforms=train_transform)\n",
        "    return DataLoader(leaves_data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7577d95e",
      "metadata": {
        "id": "7577d95e"
      },
      "source": [
        "### Get Class Labels\n",
        "\n",
        "`get_labels` simply converts the numerical labeling produced by the model into the actual class label names (e.g. \"Healthy\" instead of 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b9a22222",
      "metadata": {
        "id": "b9a22222"
      },
      "outputs": [],
      "source": [
        "def get_labels(\n",
        "    y: np.ndarray[typing.Any, typing.Any],\n",
        "    classes: list[str] = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"],\n",
        ") -> np.ndarray[typing.Any, typing.Any]:\n",
        "    fxn = lambda x: classes[x]\n",
        "    applyall = np.vectorize(fxn)\n",
        "    return applyall(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34f68c8",
      "metadata": {
        "id": "e34f68c8"
      },
      "source": [
        "## Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b01d1c",
      "metadata": {
        "id": "b4b01d1c"
      },
      "outputs": [],
      "source": [
        "# Load the images to predict\n",
        "imgs_to_pred, filenames = load_images(img_dir)\n",
        "dataloader = preprocess_img(imgs_to_pred)\n",
        "\n",
        "# Make predictions\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        preds = loaded_model(batch)\n",
        "        predicted_classes = torch.argmax(preds, dim=1)\n",
        "        all_preds.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "# Turn into actual labels\n",
        "final_labels = get_labels(all_preds)\n",
        "\n",
        "# Save as csv file\n",
        "with open('../predictions/group8_dino_predictions.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"image_filename\", \"predicted_label\"])\n",
        "    writer.writerows(zip(filenames, final_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c488e8",
      "metadata": {},
      "source": [
        "## Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "565a4588",
      "metadata": {
        "id": "565a4588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "301 301\n"
          ]
        }
      ],
      "source": [
        "print(len(filenames), len(final_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede15787",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "image_filename",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "predicted_label",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "c7337e31-df04-411b-9f3c-3e1d74a78734",
              "rows": [
                [
                  "0",
                  "0.jpeg",
                  "Pest"
                ],
                [
                  "1",
                  "1.jpeg",
                  "Healthy"
                ],
                [
                  "2",
                  "10.jpeg",
                  "Healthy"
                ],
                [
                  "3",
                  "100.jpeg",
                  "Fungi"
                ],
                [
                  "4",
                  "101.jpeg",
                  "Fungi"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_filename</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpeg</td>\n",
              "      <td>Pest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpeg</td>\n",
              "      <td>Healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.jpeg</td>\n",
              "      <td>Healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100.jpeg</td>\n",
              "      <td>Fungi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>101.jpeg</td>\n",
              "      <td>Fungi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_filename predicted_label\n",
              "0         0.jpeg            Pest\n",
              "1         1.jpeg         Healthy\n",
              "2        10.jpeg         Healthy\n",
              "3       100.jpeg           Fungi\n",
              "4       101.jpeg           Fungi"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(301, 2)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "predicted_label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "count",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "80ebe497-3a86-4bb2-a7e6-e32e33651109",
              "rows": [
                [
                  "Fungi",
                  "76"
                ],
                [
                  "Pest",
                  "67"
                ],
                [
                  "Bacteria",
                  "56"
                ],
                [
                  "Virus",
                  "48"
                ],
                [
                  "Phytopthora",
                  "33"
                ],
                [
                  "Healthy",
                  "21"
                ]
              ],
              "shape": {
                "columns": 1,
                "rows": 6
              }
            },
            "text/plain": [
              "predicted_label\n",
              "Fungi          76\n",
              "Pest           67\n",
              "Bacteria       56\n",
              "Virus          48\n",
              "Phytopthora    33\n",
              "Healthy        21\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv(\"../predictions/group8_dino_predictions.csv\")\n",
        "\n",
        "display(df.head(), df.shape)\n",
        "display(df['predicted_label'].value_counts())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs180-proj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
