{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bf854fff",
      "metadata": {
        "id": "bf854fff"
      },
      "source": [
        "# Traditional Approach Demo Code\n",
        "\n",
        "This notebook will contain the code necessary to generate the predictions file using preprocessing techniques and model generated from the `develop_deep.ipynb` notebook.\n",
        "\n",
        "For this, a deep learning model is used with DINOv2 as the model used for feature extraction for image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc91b269",
      "metadata": {
        "id": "fc91b269"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb79d32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eb79d32",
        "outputId": "bccadc83-b121-422b-c6c8-efe55260bac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pickle5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pickle5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %pip install tensorflow transformers torch torchvision torchaudio scikit-learn opencv-python numpy pickle5 tqdm -q\n",
        "\n",
        "# To hide warnings produced by different packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89f8eba",
      "metadata": {
        "id": "e89f8eba"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099fc2f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "099fc2f1",
        "outputId": "7232408b-5b55-42a3-d9c2-6aebd953d52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from transformers import AutoImageProcessor, Dinov2Model\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from copy import deepcopy\n",
        "import typing\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "import cv2.typing as cv_typing\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import csv\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72db4e6",
      "metadata": {
        "id": "e72db4e6"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b084a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b084a2",
        "outputId": "3333aaa9-80fd-4131-8e62-40759e11d059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c49dc1150b0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classes = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"]\n",
        "\n",
        "# Path of where the train images are located\n",
        "# img_dir = \"/content/drive/MyDrive/DCS/CS180/Project/potato_test\"\n",
        "img_dir = \"../data/potato_test\" # if local\n",
        "\n",
        "# Path for final model\n",
        "# model_dir = \"/content/drive/MyDrive/DCS/CS180/Project/models\"\n",
        "model_dir = \"../models\" # if local\n",
        "\n",
        "# Other constants\n",
        "ORIG_IMG_SIZE = (1500,1500)\n",
        "BATCH_SIZE = 8\n",
        "seed_value = 42\n",
        "RESIZE_IMG = (420, 420)\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PObp4aCxBkos",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PObp4aCxBkos",
        "outputId": "e63dee12-b853-45bd-b225-75723421ac3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dab0f39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dab0f39",
        "outputId": "26a28784-6213-4779-c9bb-d94907d6300a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
            "100%|██████████| 84.2M/84.2M [00:00<00:00, 162MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x NestedTensorBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): MemEffAttention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the vits14 version of dinov2\n",
        "dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "dino_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N15saeTzVv1a",
      "metadata": {
        "id": "N15saeTzVv1a"
      },
      "outputs": [],
      "source": [
        "class DinoVisionTransformerClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DinoVisionTransformerClassifier, self).__init__()\n",
        "        self.transformer = deepcopy(dino_model)\n",
        "        # self.classifier = nn.Sequential(nn.Linear(384, 384), nn.ReLU(), nn.Linear(384, 1))\n",
        "        self.classifier = nn.Sequential(nn.Dropout(0.5), nn.ReLU(), nn.Linear(in_features=384, out_features=len(classes), bias=True))\n",
        "        # self.classifier = nn.Linear(in_features=384, out_features=len(classes), bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)\n",
        "        x = self.transformer.norm(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa83af65",
      "metadata": {
        "id": "aa83af65"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "with open(Path(f\"{model_dir}/model-3.pth\"), 'rb') as file:\n",
        "    torch.serialization.add_safe_globals([DinoVisionTransformerClassifier])\n",
        "    loaded_model = torch.load(f\"{model_dir}/model-3.pth\", map_location=device, weights_only=False)\n",
        "    loaded_model = loaded_model.to(device)\n",
        "    loaded_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9be6e4",
      "metadata": {
        "id": "db9be6e4"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904fd91b",
      "metadata": {
        "id": "904fd91b"
      },
      "source": [
        "### Loading Data\n",
        "\n",
        "`load_images` takes the directory where the test images are located and loads them into a program as a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1460c1d8",
      "metadata": {
        "id": "1460c1d8"
      },
      "outputs": [],
      "source": [
        "def load_images(\n",
        "    file_path: str = \"./potato_test\",\n",
        "    resize_dim: tuple[int, int] = (518, 518),\n",
        ") -> list[cv_typing.MatLike]:\n",
        "    # Get folder\n",
        "    dir = Path(file_path)\n",
        "\n",
        "    # Check if directory\n",
        "    if not dir.is_dir():\n",
        "        raise Exception(\"Please enter a valid directory\")\n",
        "\n",
        "    # Get all images in the dir\n",
        "    imgs = [os.path.join(dir, f) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
        "\n",
        "    # Variable for final array\n",
        "    final_imgs: list[cv_typing.MatLike] = []\n",
        "    final_filenames: list[str] = []\n",
        "\n",
        "    try:\n",
        "        for img_path in imgs:\n",
        "            img_loaded: Image.Image = image.load_img(img_path, target_size=ORIG_IMG_SIZE)\n",
        "            img_array: np.ndarray[typing.Any, typing.Any] = image.img_to_array(img_loaded)\n",
        "            img_array = cv2.resize(img_array, resize_dim)\n",
        "            final_imgs.append(img_array)\n",
        "\n",
        "            filename = Path(img_path).name\n",
        "            final_filenames.append(filename)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load images: {e}\")\n",
        "\n",
        "    return final_imgs, final_filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a10643c",
      "metadata": {
        "id": "3a10643c"
      },
      "source": [
        "### Preprocessing through Feature Extraction\n",
        "\n",
        "The `preprocess_img` takes a list of images and preprocesses them using DINOv2 by taking the features of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zes8qAAQIIEl",
      "metadata": {
        "id": "Zes8qAAQIIEl"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(RESIZE_IMG),\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # Random flip with 50% probability\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.4846, 0.5446, 0.3977], std=[0.2117, 0.1950, 0.2329]),\n",
        "])\n",
        "\n",
        "class PotatoLeafDisease(Dataset):\n",
        "    def __init__(self, imgs: list[np.ndarray], transforms: transforms.Compose):\n",
        "        self.imgs = imgs\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        img = self.imgs[idx]\n",
        "\n",
        "        # Convert numpy image to PIL Image\n",
        "        image = Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "        # Apply transform\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb1ac5b7",
      "metadata": {
        "id": "eb1ac5b7"
      },
      "outputs": [],
      "source": [
        "def preprocess_img(\n",
        "    imgs: list[np.ndarray[typing.Any, typing.Any]] = [],\n",
        ") -> DataLoader:\n",
        "    all_features = []\n",
        "\n",
        "    leaves_data = PotatoLeafDisease(imgs, transforms=train_transform)\n",
        "    return DataLoader(leaves_data, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NxCcrZi-Grqi",
      "metadata": {
        "id": "NxCcrZi-Grqi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jZ5ZPCv8-e1d",
      "metadata": {
        "id": "jZ5ZPCv8-e1d"
      },
      "outputs": [],
      "source": [
        "# # Load the vits14 version of dinov2\n",
        "# dino_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "# dino_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7577d95e",
      "metadata": {
        "id": "7577d95e"
      },
      "source": [
        "### Get Class Labels\n",
        "\n",
        "`get_labels` simply converts the numerical labeling produced by the model into the actual class label names (e.g. \"Healthy\" instead of 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a22222",
      "metadata": {
        "id": "b9a22222"
      },
      "outputs": [],
      "source": [
        "def get_labels(\n",
        "    y: np.ndarray[typing.Any, typing.Any],\n",
        "    classes: list[str] = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"],\n",
        ") -> np.ndarray[typing.Any, typing.Any]:\n",
        "    fxn = lambda x: classes[x]\n",
        "    applyall = np.vectorize(fxn)\n",
        "    return applyall(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34f68c8",
      "metadata": {
        "id": "e34f68c8"
      },
      "source": [
        "## Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b01d1c",
      "metadata": {
        "id": "b4b01d1c"
      },
      "outputs": [],
      "source": [
        "# Load the images to predict\n",
        "imgs_to_pred, filenames = load_images(img_dir)\n",
        "\n",
        "# Preprocess images using DINOv2\n",
        "dataloader = preprocess_img(imgs_to_pred)\n",
        "\n",
        "# Make predictions\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        preds = loaded_model(batch)\n",
        "        predicted_classes = torch.argmax(preds, dim=1)\n",
        "        all_preds.extend(predicted_classes.cpu().numpy())\n",
        "\n",
        "# Turn into actual labels\n",
        "final_labels = get_labels(all_preds)\n",
        "\n",
        "# Save as text file\n",
        "# with open(Path(\"/content/drive/MyDrive/DCS/CS180/Project/predictions/pred_trad.csv\"), \"w\") as file:\n",
        "with open('../predictions/pred_trad.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"image_filename\", \"predicted_label\"])\n",
        "    writer.writerows(zip(filenames, final_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "565a4588",
      "metadata": {
        "id": "565a4588"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
