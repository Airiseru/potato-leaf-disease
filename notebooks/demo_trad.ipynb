{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf854fff",
   "metadata": {},
   "source": [
    "# Traditional Approach Demo Code\n",
    "\n",
    "This notebook will contain the code necessary to generate the predictions file using preprocessing techniques and model generated from the `develop_trad.ipynb` notebook.\n",
    "\n",
    "For this, a multiclass SVM is used with DINOv2 as the model used for feature extraction for image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91b269",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb79d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow transformers torch torchvision torchaudio scikit-learn opencv-python numpy pickle5 tqdm -q\n",
    "\n",
    "# To hide warnings produced by different packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f8eba",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.typing as cv_typing\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72db4e6",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43b084a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19d1c8aa8f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classes = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"]\n",
    "\n",
    "# Path of where the train images are located\n",
    "# img_dir = \"/content/drive/MyDrive/BSCS/CS180/Project/potato_test\"\n",
    "img_dir = \"../data/potato_test\" # if local\n",
    "\n",
    "# Path for final model\n",
    "# model_dir = \"/content/drive/MyDrive/BSCS/CS180/Project/models\"\n",
    "model_dir = \"../models\" # if local\n",
    "\n",
    "# Other constants\n",
    "ORIG_IMG_SIZE = (1500,1500)\n",
    "BATCH_SIZE = 8\n",
    "seed_value = 42\n",
    "\n",
    "# Set seed\n",
    "tf.random.set_seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dab0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_fast` is set to `True` but the image processor class does not have a fast version.  Falling back to the slow version.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dinov2Model(\n",
       "  (embeddings): Dinov2Embeddings(\n",
       "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "      (projection): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): Dinov2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x Dinov2Layer(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attention): Dinov2Attention(\n",
       "          (attention): Dinov2SelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): Dinov2SelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_scale1): Dinov2LayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Dinov2MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_scale2): Dinov2LayerScale()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dinov2 processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-large\", use_fast=True)\n",
    "dino_model = Dinov2Model.from_pretrained(\"facebook/dinov2-large\").to(device)\n",
    "dino_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa83af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open(Path(f\"{model_dir}/svm_model_final.pkl\"), 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9be6e4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fd91b",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "`load_images` takes the directory where the test images are located and loads them into a program as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1460c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(\n",
    "    file_path: str = \"./potato_test\",\n",
    "    resize_dim: tuple[int, int] = (518, 518),\n",
    ") -> list[cv_typing.MatLike]:\n",
    "    # Get folder\n",
    "    dir = Path(file_path)\n",
    "\n",
    "    # Check if directory\n",
    "    if not dir.is_dir():\n",
    "        raise Exception(\"Please enter a valid directory\")\n",
    "\n",
    "    # Get all images in the dir\n",
    "    imgs = [os.path.join(dir, f) for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]\n",
    "\n",
    "    # Variable for final array\n",
    "    final_imgs: list[cv_typing.MatLike] = []\n",
    "\n",
    "    try:\n",
    "        for img_path in imgs:\n",
    "            img_loaded: Image.Image = image.load_img(img_path, target_size=ORIG_IMG_SIZE)\n",
    "            img_array: np.ndarray[typing.Any, typing.Any] = image.img_to_array(img_loaded)\n",
    "            img_array = cv2.resize(img_array, resize_dim)\n",
    "            final_imgs.append(img_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load images: {e}\")\n",
    "    \n",
    "    return final_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10643c",
   "metadata": {},
   "source": [
    "### Preprocessing through Feature Extraction\n",
    "\n",
    "The `preprocess_img` takes a list of images and preprocesses them using DINOv2 by taking the features of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb1ac5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(\n",
    "    imgs: list[np.ndarray[typing.Any, typing.Any]] = [],   \n",
    ") -> np.ndarray[typing.Any, typing.Any]:\n",
    "    all_features = []\n",
    "\n",
    "    # Split into batches\n",
    "    for i in tqdm(range(0, len(imgs), BATCH_SIZE)):\n",
    "        batch = imgs[i:i+BATCH_SIZE]\n",
    "\n",
    "        # Limit pixel values and convert each image to PIL\n",
    "        batch = [Image.fromarray(np.clip(img, 0, 255).astype(np.uint8)) for img in batch]\n",
    "\n",
    "        # Preprocess and move to GPU\n",
    "        print(f\"\\tProcesing batch {i}\")\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "        print(\"\\tFinished processing the batch\")\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "            features = outputs.pooler_output\n",
    "            print(\"\\tFeatures taken using  DINOv2\")\n",
    "\n",
    "        all_features.append(features.cpu().numpy())\n",
    "        print(\"\\tCompleted all batches for  DINOv2\\n\")\n",
    "\n",
    "    # Combine all batches into one\n",
    "    return np.vstack(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577d95e",
   "metadata": {},
   "source": [
    "### Get Class Labels\n",
    "\n",
    "`get_labels` simply converts the numerical labeling produced by the model into the actual class label names (e.g. \"Healthy\" instead of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9a22222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(\n",
    "    y: np.ndarray[typing.Any, typing.Any],\n",
    "    classes: list[str] = [\"Bacteria\", \"Fungi\", \"Healthy\", \"Pest\", \"Phytopthora\", \"Virus\"],\n",
    ") -> np.ndarray[typing.Any, typing.Any]:\n",
    "    fxn = lambda x: classes[x]\n",
    "    applyall = np.vectorize(fxn)\n",
    "    return applyall(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f68c8",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b01d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcesing batch 0\n",
      "\tFinished processing the batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeatures taken using  DINOv2\n",
      "\tCompleted all batches for  DINOv2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the images to predict\n",
    "imgs_to_pred = load_images(img_dir)\n",
    "\n",
    "# Preprocess images using DINOv2\n",
    "processed_img = preprocess_img(imgs_to_pred)\n",
    "\n",
    "# Make predictions\n",
    "preds = loaded_model.predict(processed_img)\n",
    "\n",
    "# Turn into actual labels\n",
    "final_labels = get_labels(preds)\n",
    "\n",
    "# Save as text file\n",
    "with open(Path(\"../predictions/pred_trad.txt\"), \"w\") as file:\n",
    "    file.write(\"\\n\".join(list(final_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs180-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
